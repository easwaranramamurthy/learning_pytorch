 15%|██████▌                                     | 3/20 [02:00<11:23, 40.23s/it]
Traceback (most recent call last):
  File "/Users/easwaranramamurthy/Desktop/git_repos/learning_pytorch/count_a_mlp.py", line 82, in <module>
    loss.backward()
    ^^^^^^^^^^^^
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/adamw.py", line 220, in step
    adamw(
    ~~~~~^
        params_with_grad,
        ^^^^^^^^^^^^^^^^^
    ...<18 lines>...
        has_complex=has_complex,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/adamw.py", line 782, in adamw
    func(
    ~~~~^
        params,
        ^^^^^^^
    ...<16 lines>...
        has_complex=has_complex,
        ^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/Users/easwaranramamurthy/miniforge3/envs/learning_pytorch/lib/python3.13/site-packages/torch/optim/adamw.py", line 429, in _single_tensor_adamw
    param.addcdiv_(exp_avg, denom, value=-step_size)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
